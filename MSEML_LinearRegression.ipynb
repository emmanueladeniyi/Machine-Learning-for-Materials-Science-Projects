{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Using a linear regression model to predict material properties*\n",
    "\n",
    "\n",
    "**Why?** Creating regression models that represent correlations between properties associated with materials enables engineers, scientists and students to find interesting trends on datasets and develop simple surrogate models.\n",
    "\n",
    "**What?** In this tutorial, we will learn how to use the linear regression model from the [Scikit-learn](https://scikit-learn.org/stable/) Python library to develop simple models for correlations between properties across materials. Data will be obtained from [Pymatgen](http://pymatgen.org/) and [Mendeleev](https://mendeleev.readthedocs.io/en/stable/).\n",
    "\n",
    "**How to use this?** This tutorial uses Python, some familiarity with programming would be beneficial but is not required. Run each code cell in order by clicking \"Shift + Enter\". Feel free to modify the code, or change queries to familiarize yourself with the workings on the code.\n",
    "\n",
    "Suggested modifications and exercises are included in <font color=blue> blue</font>.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Getting data\n",
    "2. Processing and Organizing Data\n",
    "3. Creating the Model\n",
    "4. Plotting\n",
    "\n",
    "\n",
    "**Get started:** Click \"Shift-Enter\" on the code cells to run! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting Data\n",
    "\n",
    "Using the queries from the [MSEML Query_Viz](MSEML_Query_Viz.ipynb) tutorial, we will create lists containing different properties of elements. In this first snippet of code we will import all relevant libraries, the elements that will be turned into cases and the properties that will serve as the attributes for the cases. The elements listed were chosen because they represent the three most common crystallographic structures, and also querying them for properties yields a dataset with no unknown values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymatgen as pymat\n",
    "import mendeleev as mendel\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fcc_elements = [\"Ag\", \"Al\", \"Au\", \"Cu\", \"Ir\", \"Ni\", \"Pb\", \"Pd\", \"Pt\", \"Rh\", \"Th\", \"Yb\"]\n",
    "bcc_elements = [\"Ba\", \"Cr\", \"Eu\", \"Fe\", \"Li\", \"Mn\", \"Mo\", \"Na\", \"Nb\", \"Ta\", \"V\", \"W\" ]\n",
    "hcp_elements = [\"Be\", \"Ca\", \"Cd\", \"Co\", \"Dy\", \"Er\", \"Gd\", \"Hf\", \"Ho\", \"Lu\", \"Mg\", \"Re\", \n",
    "                \"Ru\", \"Sc\", \"Tb\", \"Ti\", \"Tl\", \"Tm\", \"Y\", \"Zn\", \"Zr\"]\n",
    "others = [\"Sb\", \"Sm\", \"Bi\", \"Ce\", \"Sn\", \"Si\"]\n",
    "# Others (Solids): \"Sb\", \"Sm\", Bi\" and \"As\" are Rhombohedral; \"C\" , \"Ce\" and \"Sn\" are Allotropic; \n",
    "# \"Si\" and \"Ge\" are Face-centered diamond-cubic;\n",
    "\n",
    "elements = fcc_elements + bcc_elements + hcp_elements + others\n",
    "\n",
    "# This function randomly arranges the elements so we can have representation for all groups both in the training and testing set\n",
    "shuffle(elements) \n",
    "\n",
    "data_youngs_modulus = []\n",
    "data_lattice_constant = []\n",
    "data_melting_point = []\n",
    "data_specific_heat = []\n",
    "data_atomic_mass = []\n",
    "data_CTE = []\n",
    "\n",
    "for item in elements:\n",
    "    data_youngs_modulus.append(pymat.Element(item).youngs_modulus)\n",
    "    data_lattice_constant.append(mendel.element(item).lattice_constant)\n",
    "    data_melting_point.append(mendel.element(item).melting_point)\n",
    "    data_specific_heat.append(mendel.element(item).specific_heat)\n",
    "    data_atomic_mass.append(pymat.Element(item).atomic_mass)\n",
    "    data_CTE.append(pymat.Element(item).coefficient_of_linear_thermal_expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see how these lists look by printing them.\n",
    "\n",
    "print(data_youngs_modulus)\n",
    "#print(data_lattice_constant)\n",
    "#print(data_melting_point)\n",
    "#print(data_specific_heat)\n",
    "#print(data_atomic_mass)\n",
    "#print(data_CTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing and Organizing Data\n",
    "\n",
    "Simple Linear Regression is one of the easiest models to implement, and can let you see how one property is related to another explanatory variable. As with all Machine Learning models, we will use a training set and a testing set.\n",
    "\n",
    "##### SETS\n",
    "\n",
    "Each of the lists we just created contains values for the properties of the elements. Analyzing the code, you can infer that the indexes in these arrays represent a specific element. For example:\n",
    "\n",
    "    For element \"Ag\" with index 2, the properties would be located at:\n",
    "   \n",
    "    data_young_modulus [2]\n",
    "    data_lattice_constant[2]\n",
    "    data_melting_point[2]\n",
    "    data_specific_heat[2]\n",
    "    data_atomic_mass[2]\n",
    "    data_CTE[2]\n",
    "\n",
    "\n",
    "##### TRAINING AND TESTING SETS\n",
    "\n",
    "With all this data, we will select 45 elements to become the training set and 6 elements to be the testing set.\n",
    "\n",
    "The training group will be used to train or develop the model, the second group will be used for testing. This is done to avoid or check overfitting. While overfitting is not likely in a linear model it can happen in more complex models, like neural networks.\n",
    "\n",
    "##### NORMALIZATION\n",
    "\n",
    "Machine Learning models usually require data to be processed and normalized. In this linear model we will process the data to be in the format required by the [sklearn](https://scikit-learn.org/stable/) model, but we will not normalize as we are interested in visualizing trends in our raw data, instead of precise predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will organize the data in a format sk-learn accepts\n",
    "# We will develop linear models that relate Young's modulus and melting temperature, CTE vs. melting temperature \n",
    "# and lattice constant and melting temperature\n",
    "\n",
    "# These would be the sets for Melting Point\n",
    "\n",
    "melt_train = data_melting_point[:45] # This takes the first 45 entries to be the Training Set\n",
    "melt_test = data_melting_point[-6:] # This takes the last 6 entries to be the Testing Set\n",
    "\n",
    "# This Reshape function in the next two lines, turns each of the horizontal lists [ x, y, z] into a\n",
    "# vertical NumPy array [[x]\n",
    "#                       [y]\n",
    "#                       [z]]\n",
    "# This Step is required to work with the Sklearn Linear Model\n",
    "\n",
    "melt_train = np.array(melt_train).reshape(-1,1) \n",
    "melt_test = np.array(melt_test).reshape(-1,1)\n",
    "\n",
    "#Each data set will be divided in training and test data\n",
    "# These would be the sets for Young's Modulus\n",
    "\n",
    "young_train = data_youngs_modulus[:45]\n",
    "young_test = data_youngs_modulus[-6:]\n",
    "young_train = np.array(young_train).reshape(-1,1)\n",
    "young_test = np.array(young_test).reshape(-1,1)\n",
    "\n",
    "# These would be the sets for Lattice Constants\n",
    "\n",
    "lattice_train = data_lattice_constant[:45]\n",
    "lattice_test = data_lattice_constant[-6:]\n",
    "lattice_train = np.array(lattice_train).reshape(-1,1)\n",
    "lattice_test = np.array(lattice_test).reshape(-1,1)\n",
    "\n",
    "# These would be the sets for Specific Heat\n",
    "\n",
    "specheat_train = data_specific_heat[:45]\n",
    "specheat_test = data_specific_heat[-6:]\n",
    "specheat_train = np.array(specheat_train).reshape(-1,1)\n",
    "specheat_test = np.array(specheat_test).reshape(-1,1)\n",
    "\n",
    "# These would be the sets for Atomic Mass\n",
    "\n",
    "mass_train = data_atomic_mass[:45]\n",
    "mass_test = data_atomic_mass[-6:]\n",
    "mass_train = np.array(mass_train).reshape(-1,1)\n",
    "mass_test = np.array(mass_test).reshape(-1,1)\n",
    "\n",
    "# These would be the sets for CTE\n",
    "\n",
    "coefTE_train = data_CTE[:45]\n",
    "coefTE_test = data_CTE[-6:]\n",
    "coefTE_train = np.array(coefTE_train).reshape(-1,1)\n",
    "coefTE_test = np.array(coefTE_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to define model, train it and make predictions\n",
    "\n",
    "Sklearn uses an \"Ordinary least squares linear regression\" for its Linear model. Implementation is described [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). We will run it will all default values. \n",
    "\n",
    "The **regression** function in the next cell creates a model. It does so by declaring a linear model, and then fitting it to the training set. We then make predictions feeding the model the X-axis testing set. \n",
    "\n",
    "We can then use the trained model (represented as an equation) to look at the variance score and understand how well our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function defines a model, trains it, and uses it to predict\n",
    "# It also outputs the linear model and information about its accuracy\n",
    "\n",
    "def regression(x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    # Define the model and train it\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    #Join train + test data \n",
    "    full_x = np.concatenate((x_train, x_test), axis=0)\n",
    "    full_y = np.concatenate((y_train, y_test), axis=0)\n",
    "    \n",
    "    # Use the model to predict the entire set of data\n",
    "    predictions = model.predict(full_x) # Make it for all values\n",
    "    \n",
    "    # Print model and mean squared error and variance score\n",
    "    print(\"Linear Equation: %.4e X + (%.4e)\"%(model.coef_, model.intercept_))\n",
    "    print(\"Mean squared error: %.4e\" % (mean_squared_error(full_y, predictions)))\n",
    "    print('Variance score: %.4f' % r2_score(full_y, predictions))    \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> **Exercise 1**. Research and provide for definition of the mean square error and variance score in terms of the linear model and data. You can find these definitions [here](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error) and [here](https://scikit-learn.org/stable/modules/model_evaluation.html#explained-variance-score).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Function for plotting data\n",
    "\n",
    "To visualize the trends that the model fit to our data, we will plot the two properties. We will use Plotly to create this plot.\n",
    "\n",
    " * The first cell defines a function that organizes data and generates the plot\n",
    " * The following cells train  models and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly #This is the library import\n",
    "import plotly.graph_objs as go # This is the graphical object (Think \"plt\" in Matplotlib if you have used that before)\n",
    "from plotly.offline import iplot # These lines are necessary to run Plotly in Jupyter Notebooks, but not in a dedicated environment\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "def plot(x_train, x_test, y_train, y_test, x_label, y_label, predictions):\n",
    "    \n",
    "    # The reshape functions in the next two lines, turns each of the\n",
    "    # vertical NumPy array [[x]\n",
    "    #                       [y]\n",
    "    #                       [z]]\n",
    "    # into python lists [ x, y, z]\n",
    "    \n",
    "    # This step is required to create plots with plotly like we did in the previous tutorial\n",
    "    \n",
    "    x_train = x_train.reshape(1,-1).tolist()[0]\n",
    "    x_test = x_test.reshape(1,-1).tolist()[0]\n",
    "    y_train = y_train.reshape(1,-1).tolist()[0]\n",
    "    y_test = y_test.reshape(1,-1).tolist()[0]    \n",
    "    predictions = predictions.reshape(1,-1).tolist()[0]\n",
    "    full_x_list = x_train + x_test\n",
    "\n",
    "    \n",
    "    # Now we get back to what we know. Remember, to plot in Plotly, we need a layout and at least one trace\n",
    "    \n",
    "    layout0= go.Layout(hovermode= 'closest', width = 800, height=600, showlegend=True,  # Hovermode establishes the way the labels that appear when you hover are arranged # Establishing a square plot width=height\n",
    "    xaxis= dict(title=go.layout.xaxis.Title(text=x_label, font=dict(size=24)), zeroline= False, gridwidth= 1, tickfont=dict(size=18)), # Axis Titles. Removing the X-axis Mark. Adding a Grid\n",
    "    yaxis= dict(title=go.layout.yaxis.Title(text=y_label, font=dict(size=24)), zeroline= False, gridwidth= 1, tickfont=dict(size=18)), # Axis Titles. Removing the Y-axis Mark. Adding a Grid\n",
    "    legend=dict(font=dict(size=24))) # Adding a legend\n",
    "    \n",
    "\n",
    "    training = go.Scatter(x = x_train, y = y_train, mode = 'markers', \n",
    "                          marker= dict(size= 10, color= 'green'), name= \"Training Data\") \n",
    "    # This trace contains the values for the data in the training set\n",
    "    \n",
    "    actual = go.Scatter(x = x_test, y = y_test, mode = 'markers', \n",
    "                        marker= dict(size= 10, color= 'red'), name= \"Testing Data\") \n",
    "    # This trace contains the values for the data in the testing set\n",
    "\n",
    "    prediction = go.Scatter(x = full_x_list, y = predictions, mode = 'lines', \n",
    "                            line = dict(color = \"blue\", width = 1.5),name= \"Model\") \n",
    "    # This trace will be the line the model fitted the data to\n",
    "\n",
    "    data = [training, actual, prediction]\n",
    "    fig= go.Figure(data, layout=layout0)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train models and plot results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regression(melt_train, melt_test, young_train, young_test) \n",
    "# This line calls the Regression model implemented in the function \n",
    "\n",
    "plot(melt_train, melt_test, young_train, young_test, \"Melting Temperature (K)\", \"Young's Modulus (GPa)\", predictions) \n",
    "# This line plots the results from that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regression(melt_train, melt_test, lattice_train, lattice_test)\n",
    "\n",
    "plot(melt_train, melt_test, lattice_train, lattice_test, \n",
    "                       \"Melting Temperature (K)\", \"Lattice Constant (Ã…)\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regression(melt_train, melt_test, coefTE_train, coefTE_test)\n",
    "\n",
    "plot(melt_train, melt_test, coefTE_train, coefTE_test, \"Melting Temperature (K)\", \"Coefficient of Linear Thermal Expansion (K<sup>-1</sup>)\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * <font color=blue> **Exercise 2**. For the three cases above. What type of correlation do you find between properties? Explain the origin of these, approximate, correlations.\n",
    " * <font color=blue> **Exercise 3**. Plot other two properties of your choice. </font>\n",
    " * <font color=blue> **Exercise 4**. Note the mean squared error in the various models. What error would you make, in average, if you used the melting temperature to predict Young's modulus? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
