{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Using neural networks to estimate Young's Modulus for elements*\n",
    "\n",
    "**Why?** Creating regression models from attributes can help scientists, engineers and students get an approximate value for unknown or unmeasured properties of materials.\n",
    "\n",
    "**What?** In this tutorial we will learn how to use neural networks from the [Keras](https://keras.io/) library to create a regression model to estimate Young's modulus.\n",
    "\n",
    "You can find another example of neural network regression using Keras in the [TensorFlow Tutorials](https://nanohub.org/tools/tftutorials) nanoHUB tool.\n",
    "\n",
    "**How to use this?** This tutorial uses Python, some familiarity with programming would be beneficial but is not required. Run each code cell in order by clicking \"Shift + Enter\". Feel free to modify the code, or change queries to familiarize yourself with the workings on the code.\n",
    "\n",
    "Suggested modifications and exercises are included in <font color=blue> blue</font>.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Getting data\n",
    "2. Processing and Organizing Data\n",
    "3. Creating the Model\n",
    "4. Plotting\n",
    "\n",
    "\n",
    "**Get started:** Click \"Shift-Enter\" on the code cells to run! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting a dataset\n",
    "\n",
    "We will repeat the process of obtaining a dataset used in the [previous](MSEML_Regression.ipynb) tutorial and the explanations are repeated for convenience.\n",
    "\n",
    "Datasets containing properties for the elements in the periodic table are available online; however, it would be thematic to create our own, using the tools from the first tutorial on [MSEML Query_Viz](MSEML_Query_Viz.ipynb). In this section we will query both [Pymatgen](http://pymatgen.org/) and [Mendeleev](https://mendeleev.readthedocs.io/en/stable/) to get a complete set of properties per element. We will use this data to create the cases from which the model will train and test.\n",
    "<br>\n",
    "<br>\n",
    "In this first snippet of code we will import all relevant libraries, the elements that will be turned into cases and the properties that will serve as the attributes for the cases. We will get 49 entries (which is a small dataset), but should give us a somewhat accurate prediction. We will also include some values to \"patch\" some unknown values in the dataset. It is important to note that more entries would move the prediction closer to the real value, and so would more attributes.\n",
    "<br>\n",
    "<br>\n",
    "The elements listed were chosen because querying them for these properties yields a dataset with few unknown values, and because they represent the three most common crystallographic structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import initializers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "import pymatgen as pymat\n",
    "import mendeleev as mendel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fcc_elements = [\"Ag\", \"Al\", \"Au\", \"Cu\", \"Ir\", \"Ni\", \"Pb\", \"Pd\", \"Pt\", \"Rh\", \"Th\", \"Yb\"]\n",
    "bcc_elements = [\"Ba\", \"Cr\", \"Cs\", \"Eu\", \"Fe\", \"Li\", \"Mn\", \"Mo\", \"Na\", \"Nb\", \"Rb\", \"Ta\", \"V\", \"W\" ]\n",
    "hcp_elements = [\"Be\", \"Ca\", \"Cd\", \"Co\", \"Dy\", \"Er\", \"Gd\", \"Hf\", \"Ho\", \"Lu\", \"Mg\", \"Re\", \n",
    "                \"Ru\", \"Sc\", \"Tb\", \"Ti\", \"Tl\", \"Tm\", \"Y\", \"Zn\", \"Zr\"]\n",
    "others = [\"Si\", \"Ge\"] # \"Si\" and \"Ge\" are Face-centered diamond-cubic;\n",
    "\n",
    "elements = fcc_elements + others + bcc_elements + hcp_elements\n",
    "\n",
    "querable_mendeleev = [\"atomic_number\", \"atomic_volume\", \"boiling_point\",\n",
    "                      \"en_ghosh\",  \"evaporation_heat\", \"heat_of_formation\",\n",
    "                     \"lattice_constant\", \"specific_heat\"]\n",
    "querable_pymatgen = [\"atomic_mass\", \"atomic_radius\", \"electrical_resistivity\",\n",
    "                     \"molar_volume\", \"bulk_modulus\", \"youngs_modulus\",\n",
    "                     \"average_ionic_radius\", \"density_of_solid\",\n",
    "                     \"coefficient_of_linear_thermal_expansion\"]\n",
    "\n",
    "querable_values = querable_mendeleev + querable_pymatgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting these values, we will proceed with our queries. Depending on the database (either Pymatgen or Mendeleev) where the property can be found, this code fills up a list with the properties of each of the elements. To visualize how the dataset we just created looks, we will use the [Pandas](https://pandas.pydata.org/) library to display it. This library will take the list of lists and show it in a nice, user-friendly table with the properties as the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_values = [] # Values for Attributes\n",
    "all_labels = [] # Values for Young's Modulus (Property to be estimated)\n",
    "\n",
    "for item in elements:\n",
    "    element_values = []\n",
    "    \n",
    "    # This section queries Mendeleev\n",
    "    element_object = mendel.element(item)\n",
    "    for i in querable_mendeleev:    \n",
    "        element_values.append(getattr(element_object,i))\n",
    "\n",
    "    # This section queries Pymatgen\n",
    "    element_object = pymat.Element(item)    \n",
    "    for i in querable_pymatgen:\n",
    "        element_values.append(getattr(element_object,i))\n",
    "        \n",
    "    all_values.append(element_values) # All lists are appended to another list, creating a list of lists\n",
    "    \n",
    "# Pandas Dataframe\n",
    "df = pd.DataFrame(all_values, columns=querable_values)\n",
    "\n",
    "# We will patch some of the values that are not available in the datasets.\n",
    "\n",
    "# Value for the CTE of Cesium\n",
    "index_Cs = df.index[df['atomic_number'] == 55]\n",
    "df.iloc[index_Cs, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000097 \n",
    "# Value from: David R. Lide (ed), CRC Handbook of Chemistry and Physics, 84th Edition. CRC Press. Boca Raton, Florida, 2003\n",
    "\n",
    "# Value for the CTE of Rubidium\n",
    "index_Rb = df.index[df['atomic_number'] == 37]\n",
    "df.iloc[index_Rb, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000090 \n",
    "# Value from: https://www.azom.com/article.aspx?ArticleID=1834\n",
    "\n",
    "# Value for the Evaporation Heat of Ruthenium\n",
    "index_Ru = df.index[df['atomic_number'] == 44]\n",
    "df.iloc[index_Ru, df.columns.get_loc(\"evaporation_heat\")] = 595 # kJ/mol \n",
    "# Value from: https://www.webelements.com/ruthenium/thermochemistry.html\n",
    "\n",
    "# Value for the Bulk Modulus of Zirconium\n",
    "index_Zr = df.index[df['atomic_number'] == 40]\n",
    "df.iloc[index_Zr, df.columns.get_loc(\"bulk_modulus\")] = 94 # GPa \n",
    "# Value from: https://materialsproject.org/materials/mp-131/\n",
    "\n",
    "# Value for the Bulk Modulus of Germanium\n",
    "index_Ge = df.index[df['atomic_number'] == 32]\n",
    "df.iloc[index_Ge, df.columns.get_loc(\"bulk_modulus\")] = 77.2 # GPa \n",
    "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge\n",
    "\n",
    "# Value for the Young's Modulus of Germanium\n",
    "index_Ge = df.index[df['atomic_number'] == 32]\n",
    "df.iloc[index_Ge, df.columns.get_loc(\"youngs_modulus\")] = 102.7 # GPa \n",
    "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge\n",
    "\n",
    "\n",
    "# The labels (values for Young's modulus) are stored separately for clarity (We drop the column later)\n",
    "\n",
    "df.to_csv(os.path.expanduser('~/mseml_data.csv'), index=False, compression=None) # this line saves the data we collected into a .csv file into your home directory\n",
    "\n",
    "all_labels = df['youngs_modulus'].tolist()\n",
    "df = df.drop(['youngs_modulus'], axis=1)\n",
    "\n",
    "df.head(n=10) # With this line you can see the first ten entries of our database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * <font color=blue> **Exercise 1.** Use the Pandas dataframe created above to plot Young's modulus vs melting temperature. Note that this is recreating the plot from previous tutorials, but using the Pandas framework to slice and access data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing and Organizing Data\n",
    "\n",
    "Most machine learning models are trained on a subset of all the available data, called the \"training set\", and the models are tested on the remainder of the available data, called the \"testing set\". Model performance has often been found to be enhanced when the inputs are normalized.\n",
    "\n",
    "##### SETS\n",
    "\n",
    "With the dataset we just created, we have 49 entries for our model. We will train with 44 cases and test on the remaining 5 elements to estimate Young's Modulus.\n",
    "\n",
    "##### NORMALIZATION\n",
    "\n",
    "Each one of these input data features has different units and is represented in scales with distinct orders of magnitude. Datasets that contain inputs like this need to be normalized, so that quantities with large values do not *overwhelm* the neural network, forcing it tune its weights to account for the different scales of our input data. In this tutorial, we will use the Standard Score Normalization, which subtracts the mean of the feature and divide by its standard deviation.\n",
    "\n",
    "<span style=\"font-size:2em;\">$ \\frac{X - µ}{σ} $ </span>\n",
    "\n",
    "While our model might converge without feature normalization, the resultant model would be difficult to train and would be dependent on the choice of units used in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will rewrite the arrays with the patches we made on the dataset by turning the dataframe back into a list of lists\n",
    "\n",
    "all_values = [list(df.iloc[x]) for x in range(len(all_values))]\n",
    "\n",
    "# SETS\n",
    "\n",
    "# List of lists are turned into Numpy arrays to facilitate calculations in steps to follow (Normalization).\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "print(\"Shape of Values:\", all_values.shape)\n",
    "all_labels = np.array(all_labels, dtype = float)\n",
    "print(\"Shape of Labels:\", all_labels.shape)\n",
    "\n",
    "# Uncomment the line below to shuffle the dataset (we do not do this here to ensure consistent results for every run)\n",
    "#order = np.argsort(np.random.random(all_labels.shape)) # This numpy argsort returns the indexes that would be used to shuffle a list\n",
    "order = np.arange(49)\n",
    "all_values = all_values[order]\n",
    "all_labels = all_labels[order]\n",
    "\n",
    "# Training Set\n",
    "train_labels = all_labels[:44]\n",
    "train_values = all_values[:44]\n",
    "\n",
    "# Testing Set\n",
    "test_labels = all_labels[-5:]\n",
    "test_values = all_values[-5:]\n",
    "\n",
    "# This line is used for labels in the plots at the end of the tutorial - Testing Set\n",
    "labeled_elements = [elements[x] for x in order[-5:]] \n",
    "elements = [elements[x] for x in order]\n",
    "\n",
    "# NORMALIZATION\n",
    "\n",
    "mean = np.mean(train_values, axis = 0) # mean\n",
    "std = np.std(train_values, axis = 0) # standard deviation\n",
    "\n",
    "train_values = (train_values - mean) / std # input scaling\n",
    "test_values = (test_values - mean) / std # input scaling\n",
    "\n",
    "print(train_values[0]) # print a sample entry from the training set\n",
    "print(test_values[0]) # print a sample entry from the training set\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Model\n",
    "\n",
    "For this regression, we will use a simple sequential neural network with one densely connected hidden layer. The optimizer used will be [RMSPropOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer) (Root Mean Square Propagation).\n",
    "\n",
    "To learn more about Root Mean Squared Propagation, click [here](https://climin.readthedocs.io/en/latest/rmsprop.html).\n",
    "\n",
    "A cool tool developed by Tensorflow to visualize how a neural network learns, and play around with its parameters, can be found here [NN Tools](https://playground.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION OF THE MODEL\n",
    "\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0)\n",
    "bias_init = initializers.Zeros()\n",
    "# In a sequential model, the first layer must specify the input shape the model will expect; \n",
    "# in this case the value is train_values.shape[1] which is the number\n",
    "# of attributes (properties) and equals 17.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "#model.add(Dense(128, activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "model.add(Dense(1, kernel_initializer=kernel_init, bias_initializer=bias_init))\n",
    "\n",
    "# DEFINITION OF THE OPTIMIZER\n",
    "\n",
    "optimizer = optimizers.RMSprop(0.002) # Root Mean Squared Propagation\n",
    "\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING \n",
    "\n",
    "This model is trained for 2000 epochs, and we record the training accuracy in the history object.\n",
    "\n",
    "One **Epoch** occurs when you pass the entire dataset through the model. One **Batch** contains a subset of the dataset that can be fed to the model at the same time. A more detailed explanation of these concepts can be found in this [blog](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9). As we have a really small dataset compared to the ones that are usually considered to be modeled by these neural networks, we are feeding all entries at the same time, so our batch is the entire dataset, and an epoch occurs when the batch is processed.\n",
    "\n",
    "This way, by plotting \"history\" we can see the evolution of the \"learning\" of the model, that is the decrease of the Mean Absolute Error. Models in Keras are fitted to the training set using the [**fit**](https://keras.io/models/model/#fit) method.\n",
    "\n",
    "The blue curve that will come up from the History object represents how the model is learning on the training data, and the orange curve represents the validation loss, which can be thought of as the way our model evaluates data that it was not trained in. This validation loss would start going up again when we start to overfit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH REAL TIME COUNTER CLASS\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + \" Training Loss: \" + \"%4f\" %logs.get('loss') + '                                       \\r') # Updates current Epoch Number\n",
    "\n",
    "EPOCHS = 2000 # Number of EPOCHS\n",
    "\n",
    "# HISTORY Object which contains how the model learned\n",
    "\n",
    "# Training Values (Properties), Training Labels (Known Young's Moduli) \n",
    "history = model.fit(train_values, train_labels, batch_size=train_values.shape[0], \n",
    "                    epochs=EPOCHS, verbose = False, shuffle=False, validation_split=0.1, callbacks=[PrintEpNum()])\n",
    "\n",
    "\n",
    "# PLOTTING HISTORY USING MATPLOTLIB\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Abs Error')\n",
    "plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),label='Loss on training set') \n",
    "plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),label = 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVING A MODEL\n",
    "\n",
    "Compiled and trained models in Keras can be saved and distributed in .h5 files using the `model.save()` method. Running the cell below will save the current model we trained, both weights and architecture to your home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.expanduser('~/model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING\n",
    "\n",
    "Models in Keras are tested using the method [**evaluate**](https://keras.io/models/model/#evaluate). This method returns the testing loss of the model and the metrics we specified when creating it,  which in our case it's the Mean Absolute Error. For the original model in this tutorial you should get a value of **29.59 GPa** for the Mean Absolute Error. This value would decrease with more training data, more attributes/features, or a different optimizer. In the case of a model that overfits, you can expect values to start increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss, mae] = model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "print(\"Testing Set Mean Absolute Error: {:2.2f} GPa\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAKING PREDICTIONS\n",
    "\n",
    "The last step in a regression model is to make predictions for values not in the training set, which are determined by the method [**predict**](https://keras.io/models/model/#predict). In the following cell we print the elements in the testing set, the real values for their Young's moduli and the predictions generated by our machine learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_values).flatten()\n",
    "\n",
    "print(\"Elements in Test Set: \", labeled_elements)\n",
    "print(\"Real Values\", list(test_labels))\n",
    "print(\"Predictions\", list(test_predictions))\n",
    "\n",
    "values = np.concatenate((train_values, test_values), axis=0) # This line joins the values together to evaluate all of them\n",
    "predictions = model.predict(values).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plotting\n",
    "\n",
    "The easiest way to see if the model did a good job estimating the Young's Modulus for the Elements is through a plot comparing Real Values with their Predictions. We will use [Plotly](https://plot.ly/python/) to create a plot like that. We covered how to plot in Plotly in the first tutorial of this tool. For values in this plot, the line (x = y) indicates a perfect match and would be the desirable result for the points. As you analyze the plot, you can hover on the points to see the data we obtained in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "layout0= go.Layout(title=go.layout.Title(text=\"Neural Network Model - Young's Modulus\", font=dict(size=28)), hovermode= 'closest', width = 1000, height=600, showlegend=True,  # Hovermode establishes the way the labels that appear when you hover are arranged # Establishing a square plot width=height\n",
    "    xaxis= dict(title=go.layout.xaxis.Title(text='Real Values (GPa)', font=dict(size=24)), zeroline= False, gridwidth= 1, tickfont=dict(size=18)), # Axis Titles. Removing the X-axis Mark. Adding a Grid\n",
    "    yaxis= dict(title=go.layout.yaxis.Title(text='Prediction (GPa)', font=dict(size=24)), zeroline= False, gridwidth= 1, tickfont=dict(size=18)), # Axis Titles. Removing the Y-axis Mark. Adding a Grid\n",
    "    legend=dict(font=dict(size=24))) # Adding a legend\n",
    "\n",
    "trace0 = go.Scatter(x = all_labels, y = predictions, mode = 'markers', marker= dict(size= 12, color= 'blue'), text= elements, name = 'Young\\'s Modulus (Training)')\n",
    "trace1 = go.Scatter(x = test_labels, y = test_predictions, mode = 'markers', marker= dict(size= 12, color= 'red'), text = labeled_elements, name = 'Young\\'s Modulus (Testing)')\n",
    "trace2 = go.Scatter(x = [0,600], y = [0,600], mode = 'lines', name = \"Match\") # This trace is the line X = Y which would indicate that the Prediction equals the real value\n",
    "\n",
    "data = [trace0, trace1, trace2]\n",
    "fig= go.Figure(data, layout=layout0)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * <font color=blue> **Exercise 2.** Compare these results for the Young's Modulus with the ones you got from the Linear Regression Tutorial. How are they different? How can you explain this difference? </font>\n",
    " <br>\n",
    " \n",
    " * <font color=blue> **Exercise 3 (Advanced).** Uncomment a line in the cell [4] to use three hidden layers in the neural network and monitor the training. Is this model better than the one with two hidden layers? </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
